{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [TensorBoard Tutorial](https://www.datacamp.com/community/tutorials/tensorboard-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages.\n",
    "import urllib.request, json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "\n",
    "from pandas_datareader import data\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Benefits of Scalar Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "    Accuracay of a given set of predictions of size (N x n_classes) and labels of size \n",
    "    (N x n_classes).\n",
    "    \"\"\"\n",
    "    return np.sum(\n",
    "        np.argmax(predictions, axis=1) == np.argmax(labels, axis=1) * \\\n",
    "        100.0 / labels.shape[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inputs, Outputs, Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "layer_ids = [\"hidden1\", \"hidden2\", \"hidden3\", \"hidden4\", \"hidden5\", \"out\"]\n",
    "layer_sizes = [784, 500, 400, 300, 200, 100, 10]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs and labels.\n",
    "train_inputs = tf.placeholder(tf.float32, \n",
    "                              shape=(batch_size, layer_sizes[0]), \n",
    "                              name=\"train_inputs\")\n",
    "\n",
    "train_labels = tf.placeholder(tf.float32, \n",
    "                              shape=(batch_size, layer_sizes[-1]),\n",
    "                              name=\"train_labels\")\n",
    "\n",
    "# Weight and bias definitions.\n",
    "for idx, lid in enumerate(layer_ids):\n",
    "\n",
    "    with tf.variable_scope(lid):\n",
    "        w = tf.get_variable(\"weights\", \n",
    "                            shape=(layer_sizes[idx], layer_sizes[idx + 1]),\n",
    "                            initializer=tf.truncated_normal_initializer(stddev=0.05))\n",
    "        b = tf.get_variable(\"bias\",\n",
    "                            shape=(layer_sizes[idx+1]),\n",
    "                            initializer=tf.random_uniform_initializer(-0.1, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Logits, Predictions, Loss and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating logits.\n",
    "h = train_inputs\n",
    "\n",
    "for lid in layer_ids:\n",
    "    with tf.variable_scope(lid, reuse=True):\n",
    "        \n",
    "        w, b = tf.get_variable(\"weights\"), tf.get_variable(\"bias\")\n",
    "        \n",
    "        if lid != \"out\":\n",
    "            h = tf.nn.relu(tf.matmul(h, w) + b, name=lid + \"_output\")\n",
    "        else:\n",
    "            h = tf.nn.xw_plus_b(h, w, b, name=lid + \"_output\")\n",
    "\n",
    "tf_predictions = tf.nn.softmax(h, name='predictions')\n",
    "\n",
    "# Calculating loss.\n",
    "tf_loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=train_labels, logits=h), name=\"loss\"\n",
    ")\n",
    "\n",
    "# Optimizer.\n",
    "tf_learning_rate = tf.placeholder(tf.float32, shape=None, name=\"learning_rate\")\n",
    "optimizer = tf.train.MomentumOptimizer(tf_learning_rate, momentum=0.9)\n",
    "grads_and_vars = optimizer.compute_gradients(tf_loss)\n",
    "tf_loss_minimize = optimizer.minimize(tf_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name scope allows you to group various summaries together. Summaries having the same \n",
    "# name_scope will be displayed on the same row.\n",
    "with tf.name_scope(\"performance\"):\n",
    "    \n",
    "    # Summaries need to be displayed whenever you need to record the loss, feed the mean loss\n",
    "    # to this placeholder.\n",
    "    tf_loss_ph = tf.placeholder(tf.float32, shape=None, name=\"loss_summary\")\n",
    "    \n",
    "    # Create a scalar summary object for the loss so it can be displayed.\n",
    "    tf_loss_summary = tf.summary.scalar(\"loss\", tf_loss_ph)\n",
    "\n",
    "    # Whenever you need to record the loss, feed the mean test accuracy to this placeholder.\n",
    "    tf_accuracy_ph = tf.placeholder(tf.float32, shape=None, name=\"accuracy_summary\")\n",
    "    \n",
    "    # Create a scalar summary object for the accuracy so it can be displayed.\n",
    "    tf_accuracy_summary = tf.summary.scalar(\"accuracy\", tf_accuracy_ph)\n",
    "\n",
    "# Gradient norm summary.\n",
    "for g, v in grads_and_vars:\n",
    "    if \"hidden5\" in v.name and \"weights\" in v.name:\n",
    "        \n",
    "        with tf.name_scope(\"gradients\"):\n",
    "            tf_last_grad_norm = tf.sqrt(tf.reduce_mean(g ** 2))\n",
    "            tf_gradnorm_summary = tf.summary.scalar(\"grad_norm\", tf_last_grad_norm)\n",
    "            break\n",
    "\n",
    "# Merge all summaries together.\n",
    "performance_summaries = tf.summary.merge([tf_loss_summary, tf_accuracy_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the neural network: Loading Data, Training, Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1006 21:04:13.190649 139798023042880 deprecation.py:323] From <ipython-input-6-d2776f5ada22>:26: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1006 21:04:13.191390 139798023042880 deprecation.py:323] From /home/alex/anaconda3/envs/machine36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1006 21:04:13.192345 139798023042880 deprecation.py:323] From /home/alex/anaconda3/envs/machine36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 21:04:13.481658 139798023042880 deprecation.py:323] From /home/alex/anaconda3/envs/machine36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1006 21:04:13.483089 139798023042880 deprecation.py:323] From /home/alex/anaconda3/envs/machine36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "W1006 21:04:13.535663 139798023042880 deprecation.py:323] From /home/alex/anaconda3/envs/machine36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Average loss in epoch 0: 0.99093\n",
      "\tAverage Valid Accuracy in epoch 0: 92.76000\n",
      "\tAverage Test Accuracy in epoch 0: 91.68000\n",
      "\n",
      "Average loss in epoch 1: 0.18361\n",
      "\tAverage Valid Accuracy in epoch 1: 95.94000\n",
      "\tAverage Test Accuracy in epoch 1: 95.66000\n",
      "\n",
      "Average loss in epoch 2: 0.11123\n",
      "\tAverage Valid Accuracy in epoch 2: 96.72000\n",
      "\tAverage Test Accuracy in epoch 2: 96.23000\n",
      "\n",
      "Average loss in epoch 3: 0.08206\n",
      "\tAverage Valid Accuracy in epoch 3: 97.22000\n",
      "\tAverage Test Accuracy in epoch 3: 96.84000\n",
      "\n",
      "Average loss in epoch 4: 0.06079\n",
      "\tAverage Valid Accuracy in epoch 4: 97.48000\n",
      "\tAverage Test Accuracy in epoch 4: 97.39000\n",
      "\n",
      "Average loss in epoch 5: 0.04403\n",
      "\tAverage Valid Accuracy in epoch 5: 97.68000\n",
      "\tAverage Test Accuracy in epoch 5: 97.54000\n",
      "\n",
      "Average loss in epoch 6: 0.03241\n",
      "\tAverage Valid Accuracy in epoch 6: 97.36000\n",
      "\tAverage Test Accuracy in epoch 6: 97.06000\n",
      "\n",
      "Average loss in epoch 7: 0.02592\n",
      "\tAverage Valid Accuracy in epoch 7: 97.88000\n",
      "\tAverage Test Accuracy in epoch 7: 97.62000\n",
      "\n",
      "Average loss in epoch 8: 0.01796\n",
      "\tAverage Valid Accuracy in epoch 8: 97.58000\n",
      "\tAverage Test Accuracy in epoch 8: 97.46000\n",
      "\n",
      "Average loss in epoch 9: 0.01306\n",
      "\tAverage Valid Accuracy in epoch 9: 97.96000\n",
      "\tAverage Test Accuracy in epoch 9: 97.51000\n",
      "\n",
      "Average loss in epoch 10: 0.01102\n",
      "\tAverage Valid Accuracy in epoch 10: 97.80000\n",
      "\tAverage Test Accuracy in epoch 10: 97.50000\n",
      "\n",
      "Average loss in epoch 11: 0.00763\n",
      "\tAverage Valid Accuracy in epoch 11: 97.98000\n",
      "\tAverage Test Accuracy in epoch 11: 97.86000\n",
      "\n",
      "Average loss in epoch 12: 0.00382\n",
      "\tAverage Valid Accuracy in epoch 12: 98.14000\n",
      "\tAverage Test Accuracy in epoch 12: 97.91000\n",
      "\n",
      "Average loss in epoch 13: 0.00174\n",
      "\tAverage Valid Accuracy in epoch 13: 98.36000\n",
      "\tAverage Test Accuracy in epoch 13: 97.93000\n",
      "\n",
      "Average loss in epoch 14: 0.00064\n",
      "\tAverage Valid Accuracy in epoch 14: 98.32000\n",
      "\tAverage Test Accuracy in epoch 14: 98.15000\n",
      "\n",
      "Average loss in epoch 15: 0.00036\n",
      "\tAverage Valid Accuracy in epoch 15: 98.28000\n",
      "\tAverage Test Accuracy in epoch 15: 98.18000\n",
      "\n",
      "Average loss in epoch 16: 0.00023\n",
      "\tAverage Valid Accuracy in epoch 16: 98.28000\n",
      "\tAverage Test Accuracy in epoch 16: 98.18000\n",
      "\n",
      "Average loss in epoch 17: 0.00018\n",
      "\tAverage Valid Accuracy in epoch 17: 98.28000\n",
      "\tAverage Test Accuracy in epoch 17: 98.18000\n",
      "\n",
      "Average loss in epoch 18: 0.00016\n",
      "\tAverage Valid Accuracy in epoch 18: 98.28000\n",
      "\tAverage Test Accuracy in epoch 18: 98.15000\n",
      "\n",
      "Average loss in epoch 19: 0.00014\n",
      "\tAverage Valid Accuracy in epoch 19: 98.30000\n",
      "\tAverage Test Accuracy in epoch 19: 98.16000\n",
      "\n",
      "Average loss in epoch 20: 0.00012\n",
      "\tAverage Valid Accuracy in epoch 20: 98.28000\n",
      "\tAverage Test Accuracy in epoch 20: 98.15000\n",
      "\n",
      "Average loss in epoch 21: 0.00011\n",
      "\tAverage Valid Accuracy in epoch 21: 98.28000\n",
      "\tAverage Test Accuracy in epoch 21: 98.18000\n",
      "\n",
      "Average loss in epoch 22: 0.00010\n",
      "\tAverage Valid Accuracy in epoch 22: 98.32000\n",
      "\tAverage Test Accuracy in epoch 22: 98.16000\n",
      "\n",
      "Average loss in epoch 23: 0.00010\n",
      "\tAverage Valid Accuracy in epoch 23: 98.26000\n",
      "\tAverage Test Accuracy in epoch 23: 98.17000\n",
      "\n",
      "Average loss in epoch 24: 0.00009\n",
      "\tAverage Valid Accuracy in epoch 24: 98.28000\n",
      "\tAverage Test Accuracy in epoch 24: 98.14000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "n_channels = 1\n",
    "n_classes = 10\n",
    "n_train = 55000\n",
    "n_valid = 5000\n",
    "n_test = 10000\n",
    "n_epochs = 25\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 # Making sure Tensorflow doesn't \n",
    "                                                         # overflow the GPU.\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "if not os.path.exists(\"summaries\"):\n",
    "    os.mkdir(\"summaries\")\n",
    "\n",
    "if not os.path.exists(os.path.join(\"summaries\", \"second\")):\n",
    "    os.mkdir(os.path.join(\"summaries\", \"second\"))\n",
    "\n",
    "summ_writer = tf.summary.FileWriter(os.path.join(\"summaries\", \"second\"), session.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "accuracy_per_epoch = []\n",
    "mnist_data = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_per_epoch = []\n",
    "    for i in range(n_train // batch_size):\n",
    "\n",
    "        # Training for one step.\n",
    "        batch = mnist_data.train.next_batch(batch_size)    # Get one batch of training data.\n",
    "        if i == 0:\n",
    "            \n",
    "            # Only for the first epoch, get the summary data.\n",
    "            # Otherwise, it can clutter the visualization.\n",
    "            l, _, gn_summ = session.run(\n",
    "                [tf_loss, tf_loss_minimize, tf_gradnorm_summary], \n",
    "                feed_dict={train_inputs: batch[0].reshape(batch_size, image_size * image_size),\n",
    "                           train_labels: batch[1],\n",
    "                           tf_learning_rate: 0.01}\n",
    "            )\n",
    "            summ_writer.add_summary(gn_summ, epoch)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Optimize with training data.\n",
    "            l, _ = session.run(\n",
    "                [tf_loss, tf_loss_minimize],\n",
    "                feed_dict={train_inputs: batch[0].reshape(batch_size, image_size * image_size),\n",
    "                           train_labels: batch[1],\n",
    "                           tf_learning_rate: 0.01}\n",
    "            )\n",
    "        \n",
    "        loss_per_epoch.append(l)\n",
    "\n",
    "    print(\"Average loss in epoch %d: %.5f\" %(epoch, np.mean(loss_per_epoch)))    \n",
    "    avg_loss = np.mean(loss_per_epoch)\n",
    "\n",
    "    # Calculate the Validation Accuracy.\n",
    "    valid_accuracy_per_epoch = []\n",
    "\n",
    "    for i in range(n_valid // batch_size):\n",
    "        valid_images, valid_labels = mnist_data.validation.next_batch(batch_size)\n",
    "        valid_batch_predictions = session.run(\n",
    "            tf_predictions, \n",
    "            feed_dict={train_inputs: valid_images.reshape(batch_size,image_size*image_size)}\n",
    "        )\n",
    "        valid_accuracy_per_epoch.append(accuracy(valid_batch_predictions, valid_labels))\n",
    "\n",
    "    mean_v_acc = np.mean(valid_accuracy_per_epoch)\n",
    "    print(\"\\tAverage Valid Accuracy in epoch %d: %.5f\"%(epoch, \n",
    "                                                        np.mean(valid_accuracy_per_epoch))\n",
    "         )\n",
    "\n",
    "    # Calculate the Test Accuracy.\n",
    "    accuracy_per_epoch = []\n",
    "    \n",
    "    for i in range(n_test // batch_size):\n",
    "        test_images, test_labels = mnist_data.test.next_batch(batch_size)\n",
    "        test_batch_predictions = session.run(\n",
    "            tf_predictions, \n",
    "            feed_dict={train_inputs: test_images.reshape(batch_size,image_size*image_size)}\n",
    "        )\n",
    "        accuracy_per_epoch.append(accuracy(test_batch_predictions, test_labels))\n",
    "\n",
    "    print(\"\\tAverage Test Accuracy in epoch %d: %.5f\\n\" %(epoch, np.mean(accuracy_per_epoch)))\n",
    "    avg_test_accuracy = np.mean(accuracy_per_epoch)\n",
    "\n",
    "    # Execute the summaries defined above.\n",
    "    summ = session.run(performance_summaries, \n",
    "                       feed_dict={tf_loss_ph: avg_loss, \n",
    "                                  tf_accuracy_ph: avg_test_accuracy}\n",
    "                      )\n",
    "\n",
    "    # Write the obtained summaries to the file, so it can be displayed in the TensorBoard.\n",
    "    summ_writer.add_summary(summ, epoch)\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](./figures/tensorboard_tutorial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Summary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond Scalars: Visualizing Histograms/Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Histogram Summaries to Visualize Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summaries need to be displayed.\n",
    "# Create a summary for each weight and each bias in each layer.\n",
    "all_summaries = []\n",
    "\n",
    "for lid in layer_ids:\n",
    "    with tf.name_scope(lid + \"_hist\"):\n",
    "        with tf.variable_scope(lid, reuse=True):\n",
    "            w, b = tf.get_variable(\"weights\"), tf.get_variable(\"bias\")\n",
    "\n",
    "            # Create a scalar summary object for the loss so it can be displayed.\n",
    "            tf_w_hist = tf.summary.histogram(\"weights_hist\", tf.reshape(w, [-1]))\n",
    "            tf_b_hist = tf.summary.histogram(\"bias_hist\", b)\n",
    "            all_summaries.extend([tf_w_hist, tf_b_hist])\n",
    "\n",
    "# Merge all parameter histogram summaries together.\n",
    "tf_param_summaries = tf.summary.merge(all_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the neural network (with Histogram Summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Average loss in epoch 0: 1.00752\n",
      "\tAverage Valid Accuracy in epoch 0: 92.24000\n",
      "\tAverage Test Accuracy in epoch 0: 92.06000\n",
      "\n",
      "Average loss in epoch 1: 0.19359\n",
      "\tAverage Valid Accuracy in epoch 1: 95.22000\n",
      "\tAverage Test Accuracy in epoch 1: 95.32000\n",
      "\n",
      "Average loss in epoch 2: 0.11883\n",
      "\tAverage Valid Accuracy in epoch 2: 97.28000\n",
      "\tAverage Test Accuracy in epoch 2: 96.96000\n",
      "\n",
      "Average loss in epoch 3: 0.08186\n",
      "\tAverage Valid Accuracy in epoch 3: 97.18000\n",
      "\tAverage Test Accuracy in epoch 3: 97.08000\n",
      "\n",
      "Average loss in epoch 4: 0.05932\n",
      "\tAverage Valid Accuracy in epoch 4: 97.48000\n",
      "\tAverage Test Accuracy in epoch 4: 97.23000\n",
      "\n",
      "Average loss in epoch 5: 0.04555\n",
      "\tAverage Valid Accuracy in epoch 5: 97.88000\n",
      "\tAverage Test Accuracy in epoch 5: 97.63000\n",
      "\n",
      "Average loss in epoch 6: 0.03505\n",
      "\tAverage Valid Accuracy in epoch 6: 97.64000\n",
      "\tAverage Test Accuracy in epoch 6: 97.36000\n",
      "\n",
      "Average loss in epoch 7: 0.02570\n",
      "\tAverage Valid Accuracy in epoch 7: 97.86000\n",
      "\tAverage Test Accuracy in epoch 7: 97.47000\n",
      "\n",
      "Average loss in epoch 8: 0.02061\n",
      "\tAverage Valid Accuracy in epoch 8: 97.82000\n",
      "\tAverage Test Accuracy in epoch 8: 97.69000\n",
      "\n",
      "Average loss in epoch 9: 0.01428\n",
      "\tAverage Valid Accuracy in epoch 9: 97.46000\n",
      "\tAverage Test Accuracy in epoch 9: 97.22000\n",
      "\n",
      "Average loss in epoch 10: 0.01241\n",
      "\tAverage Valid Accuracy in epoch 10: 97.60000\n",
      "\tAverage Test Accuracy in epoch 10: 97.55000\n",
      "\n",
      "Average loss in epoch 11: 0.00978\n",
      "\tAverage Valid Accuracy in epoch 11: 97.74000\n",
      "\tAverage Test Accuracy in epoch 11: 97.46000\n",
      "\n",
      "Average loss in epoch 12: 0.00784\n",
      "\tAverage Valid Accuracy in epoch 12: 98.14000\n",
      "\tAverage Test Accuracy in epoch 12: 97.99000\n",
      "\n",
      "Average loss in epoch 13: 0.00935\n",
      "\tAverage Valid Accuracy in epoch 13: 98.04000\n",
      "\tAverage Test Accuracy in epoch 13: 97.52000\n",
      "\n",
      "Average loss in epoch 14: 0.00567\n",
      "\tAverage Valid Accuracy in epoch 14: 97.68000\n",
      "\tAverage Test Accuracy in epoch 14: 97.72000\n",
      "\n",
      "Average loss in epoch 15: 0.00356\n",
      "\tAverage Valid Accuracy in epoch 15: 98.36000\n",
      "\tAverage Test Accuracy in epoch 15: 98.08000\n",
      "\n",
      "Average loss in epoch 16: 0.00111\n",
      "\tAverage Valid Accuracy in epoch 16: 98.48000\n",
      "\tAverage Test Accuracy in epoch 16: 98.19000\n",
      "\n",
      "Average loss in epoch 17: 0.00042\n",
      "\tAverage Valid Accuracy in epoch 17: 98.44000\n",
      "\tAverage Test Accuracy in epoch 17: 98.13000\n",
      "\n",
      "Average loss in epoch 18: 0.00022\n",
      "\tAverage Valid Accuracy in epoch 18: 98.44000\n",
      "\tAverage Test Accuracy in epoch 18: 98.17000\n",
      "\n",
      "Average loss in epoch 19: 0.00015\n",
      "\tAverage Valid Accuracy in epoch 19: 98.40000\n",
      "\tAverage Test Accuracy in epoch 19: 98.15000\n",
      "\n",
      "Average loss in epoch 20: 0.00013\n",
      "\tAverage Valid Accuracy in epoch 20: 98.40000\n",
      "\tAverage Test Accuracy in epoch 20: 98.17000\n",
      "\n",
      "Average loss in epoch 21: 0.00011\n",
      "\tAverage Valid Accuracy in epoch 21: 98.32000\n",
      "\tAverage Test Accuracy in epoch 21: 98.17000\n",
      "\n",
      "Average loss in epoch 22: 0.00010\n",
      "\tAverage Valid Accuracy in epoch 22: 98.36000\n",
      "\tAverage Test Accuracy in epoch 22: 98.18000\n",
      "\n",
      "Average loss in epoch 23: 0.00009\n",
      "\tAverage Valid Accuracy in epoch 23: 98.36000\n",
      "\tAverage Test Accuracy in epoch 23: 98.15000\n",
      "\n",
      "Average loss in epoch 24: 0.00008\n",
      "\tAverage Valid Accuracy in epoch 24: 98.36000\n",
      "\tAverage Test Accuracy in epoch 24: 98.16000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "n_channels = 1\n",
    "n_classes = 10\n",
    "n_train = 55000\n",
    "n_valid = 5000\n",
    "n_test = 10000\n",
    "n_epochs = 25\n",
    "\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 # Making sure Tensorflow doesn't \n",
    "                                                         # overflow the GPU.\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "if not os.path.exists(\"summaries\"):\n",
    "    os.mkdir(\"summaries\")\n",
    "if not os.path.exists(os.path.join(\"summaries\", \"third\")):\n",
    "    os.mkdir(os.path.join(\"summaries\", \"third\"))\n",
    "\n",
    "summ_writer_3 = tf.summary.FileWriter(os.path.join(\"summaries\", \"third\"), session.graph)\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "accuracy_per_epoch = []\n",
    "mnist_data = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    loss_per_epoch = []\n",
    "    for i in range(n_train//batch_size):\n",
    "\n",
    "        # Training for one step.\n",
    "        batch = mnist_data.train.next_batch(batch_size) # Get one batch of training data.\n",
    "        if i == 0:\n",
    "            # Only for the first epoch, get the summary data.\n",
    "            # Otherwise, it can clutter the visualization.\n",
    "            l, _, gn_summ, wb_summ = \\\n",
    "                session.run([tf_loss, \n",
    "                             tf_loss_minimize,\n",
    "                             tf_gradnorm_summary, \n",
    "                             tf_param_summaries],\n",
    "                    feed_dict={train_inputs: batch[0].reshape(batch_size,image_size*image_size),\n",
    "                               train_labels: batch[1],\n",
    "                               tf_learning_rate: 0.00001}\n",
    "                           )\n",
    "            summ_writer_3.add_summary(gn_summ, epoch)\n",
    "            summ_writer_3.add_summary(wb_summ, epoch)\n",
    "        else:\n",
    "            # Optimize with training data.\n",
    "            l, _ = session.run([tf_loss, \n",
    "                               tf_loss_minimize],\n",
    "                              feed_dict={train_inputs: batch[0].\\\n",
    "                                         reshape(batch_size, image_size * image_size),\n",
    "                                         train_labels: batch[1],\n",
    "                                         tf_learning_rate: 0.01}\n",
    "                             )\n",
    "        loss_per_epoch.append(l)\n",
    "\n",
    "    print(\"Average loss in epoch %d: %.5f\" %(epoch, np.mean(loss_per_epoch)))    \n",
    "    avg_loss = np.mean(loss_per_epoch)\n",
    "\n",
    "    # Calculate the Validation Accuracy.\n",
    "    valid_accuracy_per_epoch = []\n",
    "    for i in range(n_valid//batch_size):\n",
    "        valid_images, valid_labels = mnist_data.validation.next_batch(batch_size)\n",
    "        valid_batch_predictions = session.run(\n",
    "            tf_predictions, feed_dict={train_inputs: valid_images.\\\n",
    "                                       reshape(batch_size,image_size * image_size)})\n",
    "        valid_accuracy_per_epoch.append(\n",
    "            accuracy(valid_batch_predictions,valid_labels)\n",
    "        )\n",
    "\n",
    "    mean_v_acc = np.mean(valid_accuracy_per_epoch)\n",
    "    print(\"\\tAverage Valid Accuracy in epoch %d: %.5f\" \\\n",
    "          %(epoch,np.mean(valid_accuracy_per_epoch)))\n",
    "\n",
    "    # Calculate the Test Accuracy.\n",
    "    accuracy_per_epoch = []\n",
    "    for i in range(n_test//batch_size):\n",
    "        test_images, test_labels = mnist_data.test.next_batch(batch_size)\n",
    "        test_batch_predictions = session.run(\n",
    "            tf_predictions,feed_dict={train_inputs: test_images.\\\n",
    "                                      reshape(batch_size,image_size*image_size)}\n",
    "        )\n",
    "        accuracy_per_epoch.append(accuracy(test_batch_predictions,test_labels))\n",
    "\n",
    "    print(\"\\tAverage Test Accuracy in epoch %d: %.5f\\n\" %\\\n",
    "          (epoch,np.mean(accuracy_per_epoch)))\n",
    "    avg_test_accuracy = np.mean(accuracy_per_epoch)\n",
    "\n",
    "    # Execute the summaries defined above.\n",
    "    summ = session.run(performance_summaries, \n",
    "                       feed_dict={tf_loss_ph:avg_loss, tf_accuracy_ph:avg_test_accuracy})\n",
    "\n",
    "    # Write the obtained summaries to the file, so they can be displayed.\n",
    "    summ_writer_3.add_summary(summ, epoch)\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Histogram Data of Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Effect of Different Initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "layer_ids = [\"hidden1\", \"hidden2\", \"hidden3\", \"hidden4\", \"hidden5\", \"out\"]\n",
    "layer_sizes = [784, 500, 400, 300, 200, 100, 10]\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs and Labels.\n",
    "train_inputs = tf.placeholder(tf.float32, shape=[batch_size, layer_sizes[0]], \n",
    "                              name=\"train_inputs\")\n",
    "train_labels = tf.placeholder(tf.float32, shape=[batch_size, layer_sizes[-1]], \n",
    "                              name=\"train_labels\")\n",
    "\n",
    "# Weight and Bias definitions.\n",
    "for idx, lid in enumerate(layer_ids):\n",
    "\n",
    "    with tf.variable_scope(lid):\n",
    "        w = tf.get_variable(\"weights\", shape=[layer_sizes[idx], layer_sizes[idx+1]],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable(\"bias\", shape=[layer_sizes[idx+1]],\n",
    "                            initializer=tf.random_uniform_initializer(-0.1, 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Logits, Predictions, Loss and Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Summaries: Visualizing Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Compare Different Initialization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution View of Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations and Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
